{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41763be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "DEPRECATION: arcgis 2.0.0 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7938549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'spark-3.5.1-bin-hadoop3.tgz'\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\fa creations\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "DEPRECATION: arcgis 2.0.0 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba823e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/Java/jdk-22/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/spark/spark-3.5.1-bin-hadoop3\"\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9d2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# Configure Spark environment variables\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Java\\\\jdk-22\"  # Adjust to your JDK path\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark\\\\spark-3.5.1-bin-hadoop3\"  # Adjust to your Spark path\n",
    "\n",
    "# Initialize findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import Spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970598b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Filter Airport Cities by Latitude\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AirportLatitudeFilter\").getOrCreate()\n",
    "\n",
    "# Read the airport data\n",
    "airport_df = spark.read.csv(\"in/cities.text\", header=False, inferSchema=True)\n",
    "\n",
    "# Filter airports with latitude > 40\n",
    "filtered_airports = airport_df.filter(airport_df._c1 > 40)\n",
    "\n",
    "# Select airport name and latitude\n",
    "result = filtered_airports.select(\"_c0\", \"_c1\")\n",
    "\n",
    "# Save the result to text file\n",
    "result.write.csv(\"out/cities_by_latitude.txt\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf12ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Find Common Hosts in NASA Logs\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CommonHosts\").getOrCreate()\n",
    "\n",
    "# Read the log files\n",
    "log1_df = spark.read.csv(\"in/nasa_log_01.tsv\", sep='\\t', header=True)\n",
    "log2_df = spark.read.csv(\"in/nasa_log_02.tsv\", sep='\\t', header=True)\n",
    "\n",
    "# Select hosts\n",
    "hosts1 = log1_df.select(\"host\").distinct()\n",
    "hosts2 = log2_df.select(\"host\").distinct()\n",
    "\n",
    "# Find common hosts\n",
    "common_hosts = hosts1.intersect(hosts2)\n",
    "\n",
    "# Save the result to CSV\n",
    "common_hosts.write.csv(\"out/same_hosts_nasa_logs.csv\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ac871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of first 100 prime numbers: 24133\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Sum of First 100 Prime Numbers\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SumPrimes\").getOrCreate()\n",
    "\n",
    "# Read the primes file\n",
    "primes_rdd = spark.sparkContext.textFile(\"in/primes.text\")\n",
    "\n",
    "# Convert each row of primes to a list of integers\n",
    "primes_rdd = primes_rdd.flatMap(lambda line: [int(x) for x in line.split()])\n",
    "\n",
    "# Sum the primes\n",
    "sum_primes = primes_rdd.sum()\n",
    "\n",
    "# Print the result\n",
    "print(\"Sum of first 100 prime numbers:\", sum_primes)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb951c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------+\n",
      "|        Location|   avg_price_sq_ft|max_price|\n",
      "+----------------+------------------+---------+\n",
      "|          Oceano|           1144.64|1195000.0|\n",
      "|         Bradley|            606.06|1600000.0|\n",
      "|     Avila Beach| 566.5500000000001|1999000.0|\n",
      "|         Cambria| 491.9558333333334|2995000.0|\n",
      "|     Pismo Beach|462.28416666666664|1799000.0|\n",
      "| San Luis Obispo|458.91333333333336|2369000.0|\n",
      "|      Santa Ynez|391.33000000000004|1395000.0|\n",
      "|         Cayucos|             386.6|1500000.0|\n",
      "|         Cayucos|            385.11| 695000.0|\n",
      "|       Morro Bay|374.13750000000005| 982800.0|\n",
      "|   Arroyo Grande| 361.4208333333333|5499000.0|\n",
      "|     Pismo Beach|         357.01125| 920000.0|\n",
      "|         Cambria| 347.5544444444444| 699900.0|\n",
      "|       Morro Bay|345.90538461538466|1100000.0|\n",
      "|         Creston|            322.75| 549000.0|\n",
      "|     Out Of Area|            314.47|1195000.0|\n",
      "|    Grover Beach|            308.78| 999000.0|\n",
      "| San Luis Obispo|291.09357142857147| 890000.0|\n",
      "|      Atascadero|            285.76| 995000.0|\n",
      "|        Lockwood|            283.33| 425000.0|\n",
      "+----------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Real Estate Data Analysis\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, max\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealEstateAnalysis\").getOrCreate()\n",
    "\n",
    "# Read the real estate data\n",
    "real_estate_df = spark.read.csv(\"in/RealEstate.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Group by location and calculate average price per SQ Ft and max price\n",
    "aggregated_df = real_estate_df.groupBy(\"Location\").agg(\n",
    "    avg(\"Price SQ Ft\").alias(\"avg_price_sq_ft\"),\n",
    "    max(\"Price\").alias(\"max_price\")\n",
    ")\n",
    "\n",
    "# Sort by average price per SQ Ft\n",
    "sorted_df = aggregated_df.orderBy(\"avg_price_sq_ft\", ascending=False)\n",
    "\n",
    "# Show the result\n",
    "sorted_df.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93f64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
